{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перетворення вхідних даних і цілей у тензори\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "# Виведення результатів\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdhi6htb8iyI",
        "outputId": "6ffea0cf-1b08-483b-e72f-4f144bf71be3"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "\n",
            "Targets:\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми перетворили масиви NumPy на тензори PyTorch та вивели вхідні дані та таргети у форматі тензорів:\n",
        "\n",
        "inputs: кожен рядок — це екземпляр з трьох вхідних характеристик для кожного спостереження.\n",
        "\n",
        "targets: кожен елемент — це мітка класу (0 або 1), яка вказує, чи кількість яблук перевищує 80."
      ],
      "metadata": {
        "id": "_STIw2Z3Hx2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26038a9-67d5-43c9-f53f-5994700cd5aa"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79c8312e5b50>"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cXjXRCTQOq2",
        "outputId": "381b0c6e-8294-45c7-aad0-d231984b15ef"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ініціалізація ваг і зміщення\n",
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# Виведення результату\n",
        "print(\"Ваги w:\\n\", w)\n",
        "print(\"\\nЗміщення b:\\n\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEBKPLjLAgX9",
        "outputId": "5110b28a-3927-4244-ece9-4965a6067f3f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ваги w:\n",
            " tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "\n",
            "Зміщення b:\n",
            " tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми ініціалізували w (ваги) як випадкові значення з нормального розподілу (за допомогою torch.randn), де розмірність ваг — це 1 рядок з 3 значеннями (для трьох вхідних характеристик).\n",
        "\n",
        "Ми також ініціалізували b (bias) як випадкове значення з нормального розподілу. Воно має розмірність 1.\n",
        "\n",
        "Наші параметри мають властивість requires_grad=True, це означає, що PyTorch буде автоматично обчислювати похідні (градієнти) для цих параметрів під час зворотного поширення помилки (backpropagation). Це необхідно для коректного оновлення ваг і зміщення (bias) за допомогою алгоритму градієнтного спуску під час тренування моделі."
      ],
      "metadata": {
        "id": "rg6K2lXXJ9BU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def model(x, w, b):\n",
        "    # Обчислюємо лінійну комбінацію\n",
        "    z = x @ w.t() + b\n",
        "\n",
        "    # Обчислюємо сигмоїду\n",
        "    y_pred = 1 / (1 + torch.exp(-z))\n",
        "\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#z=inputs @ w.t() + b"
      ],
      "metadata": {
        "id": "WggthYxETT_p"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wPqNVakTYDd",
        "outputId": "3f94e54b-c557-4dd7-9c7a-7703890786f7"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[69.4361],\n",
              "        [88.2410],\n",
              "        [97.5041],\n",
              "        [81.8390],\n",
              "        [76.1967]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Передбачення\n",
        "y_pred = model(inputs, w, b)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrG_olrcMM3l",
        "outputId": "b7de3094-d956-428c-8558-42de2b39c870"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель на цьому етапі передбачає, що всі приклади належать до класу 1. Це свідчить про те, що початкові випадково задані ваги w та зміщення b дали великі додатні значення z. У результаті сигмоїдна функція, яка різко наближається до 1 при великих позитивних вхідних значеннях, видає передбачення, близькі до 1 для всіх прикладів.\n"
      ],
      "metadata": {
        "id": "uzZL2LamW9rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    # Додаємо мале число (epsilon), щоб уникнути log(0)\n",
        "    eps = 1e-7\n",
        "    predicted_probs = torch.clamp(predicted_probs, eps, 1 - eps)\n",
        "\n",
        "    # Формула бінарної крос-ентропії\n",
        "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "\n",
        "    # Вертаємо середнє значення втрат\n",
        "    return loss.mean()\n"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(y_pred, targets)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccy39u2pfbzF",
        "outputId": "3b14c42a-78eb-4b20-f478-7920c135da0f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 6.376954078674316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наш код обчислює втрати за допомогою функції бінарної крос-ентропії для логістичної регресії.\n",
        "\n",
        "Функція виводить середнє значення втрат, що становить 6.37.\n",
        "\n",
        "На даному етапі тренування модель робить помилки при прогнозуванні, які є досить великими."
      ],
      "metadata": {
        "id": "IqFDR7wnM31_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислимо gradients\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Градієнти вагів\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at0rI-mUhjmx",
        "outputId": "fe048e8b-5e14-44ee-964c-d4c0f061fdb2"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([[0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Градієнти для bias\n",
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6JkdjNdiHw-",
        "outputId": "c9e71d47-7132-4c4d-d583-66ddaff77b2a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6213], requires_grad=True)\n",
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми обчислили градієнти для параметрів моделі (ваг і зміщень) на основі функції втрат. Оскільки градієнти рівні нулю, це означає, що модель не оновлює свої параметри під час тренування. Модель не може ефективно навчатися."
      ],
      "metadata": {
        "id": "oQRqm7JBOPaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(inputs, w, b)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd26211a-a359-4e52-c34a-fc121e29d103"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(y_pred, targets)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyiZrGSaknnu",
        "outputId": "82390555-1792-4d7c-e2bd-40612338c5cb"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6829456686973572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислимо gradients\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "YCyNFmXElyJ1"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Градієнти вагів\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiWFGk_FmJZo",
        "outputId": "75c30d76-ef2f-4d1d-fe98-f724f9c93841"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "tensor([[ -5.4417, -18.9853, -10.0682]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Градієнти для bias\n",
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSO49Au3mB5B",
        "outputId": "e5bb5b15-e612-4ac6-c721-655d0fa9b1d7"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0006], requires_grad=True)\n",
            "tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми зробили значення ваг дуже малими, при цьому вони залишилися тензорами, для яких PyTorch відслідковує градієнти.\n",
        "\n",
        "Градієнти ваг стали великими й від’ємними (-5.4417, -18.9853, -10.0682), що вказує на напрямок, у якому модель має змінювати ваги, щоб зменшити втрати.\n",
        "\n",
        "Зміщення b також має мале значення (0.0006), а його градієнт становить -0.0794.\n",
        "\n",
        "Після зменшення ваг в 1000 разів, сума втрат значно зменшилась (з ~6.38 до ~0.68), що вказує на покращення роботи моделі. Модель тепер близька до рандомного вгадування (передбачення біля 0.5), але вже не \"зламана\", як було раніше.\n"
      ],
      "metadata": {
        "id": "li7hI_3ljjUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Генеруємо передбачення\n",
        "preds = model(inputs, w, b)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxQs5i_-gDpp",
        "outputId": "c8cad1ff-d0a6-463a-c988-b0e514f76b49"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислюємо loss\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_qYfvu6gGbr",
        "outputId": "2b6fb5a4-6de7-4974-ce5e-115cc2a2653a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6829456686973572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислюємо градієнти\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "ceEhAyFMgJvi"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Оновлюємо ваги і ресетимо градієнти\n",
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()"
      ],
      "metadata": {
        "id": "qodjEYRggOy1"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вихідні дані та ваги у нас вже є:\n",
        "# inputs, targets, w, b, model(), binary_cross_entropy()\n",
        "\n",
        "# Встановимо кількість епох і швидкість навчання\n",
        "num_epochs = 1000\n",
        "learning_rate = 1e-5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Передбачення\n",
        "    y_pred = model(inputs, w, b)\n",
        "\n",
        "    # Обчислення втрат\n",
        "    loss = binary_cross_entropy(y_pred, targets)\n",
        "\n",
        "    # Обчислення градієнтів\n",
        "    loss.backward()\n",
        "\n",
        "    # Оновлення ваг\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Обнулення градієнтів\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # кожні 100 епох виводимо прогрес\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "mObHPyE06qsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c35ea4-12b6-4b34-ad01-10778b1ac8ed"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: 0.5619\n",
            "Epoch 200, Loss: 0.5066\n",
            "Epoch 300, Loss: 0.4646\n",
            "Epoch 400, Loss: 0.4324\n",
            "Epoch 500, Loss: 0.4071\n",
            "Epoch 600, Loss: 0.3869\n",
            "Epoch 700, Loss: 0.3705\n",
            "Epoch 800, Loss: 0.3569\n",
            "Epoch 900, Loss: 0.3454\n",
            "Epoch 1000, Loss: 0.3357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In0z4HarnaJb",
        "outputId": "ea3a6575-8afc-409e-dce2-1499542708ec"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5777],\n",
              "        [0.6685],\n",
              "        [0.9113],\n",
              "        [0.1616],\n",
              "        [0.8653]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Округлимо ймовірності, бо нам потрібні класи 0 або 1\n",
        "predicted_classes = (y_pred >= 0.5).float()\n",
        "print(\"Передбачені класи:\\n\", predicted_classes)\n",
        "print(\"Справжні класи (targets):\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNyh4uMxnh9L",
        "outputId": "21c59f78-c171-40a4-c575-ce1d062fb3d7"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Передбачені класи:\n",
            " tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Справжні класи (targets):\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми ініціалізували тренувальний цикл на 1000 епох із маленьким коефіцієнтом навчання (learning_rate = 1e-5). Як бачимо модель правильно класифікувала 4 з 5 прикладів. ТОбто модель навчилася розрізняти більшість прикладів і робить адекватні передбачення."
      ],
      "metadata": {
        "id": "b-5E13MWpY8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Перетворення в тензори\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "_znZuNhGpcz2"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Створення датасету\n",
        "train_ds = TensorDataset(inputs, targets)"
      ],
      "metadata": {
        "id": "jKkDE-uQpohY"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYtkIgfirSkQ",
        "outputId": "beb96477-1233-4c0d-db92-c04d398e407a"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми підготували дані для подальшого навчання моделі: конвертували inputs і targets у тензори та створили датасет train_ds за допомогою класу TensorDataset."
      ],
      "metadata": {
        "id": "Yv9XacejqtP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Створення DataLoader\n",
        "train_dl = DataLoader(train_ds, batch_size=5, shuffle=True)\n",
        "\n",
        "# Виведення першого батчу\n",
        "for batch in train_dl:\n",
        "    inputs_batch, targets_batch = batch\n",
        "    print(inputs_batch)\n",
        "    print(targets_batch)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67ac1a4-9a85-419c-e1c2-d5abeeb72e8f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.],\n",
            "        [102.,  43.,  37.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми підготували механізм для пакетної обробки даних, увімкнули shuffle=True, щоб дані випадково перемішувалися перед кожною епохою навчання, це допоможе уникнути залежності від порядку даних. Дані будуть подаватися в модель невеликими порціями (батчами), що дозволить ефективніше і стабільніше тренувати модель."
      ],
      "metadata": {
        "id": "96bXUdPmrmGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Клас логістичної регресії\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.linear = nn.Linear(3, 1)  # 3 входи, 1 вихід\n",
        "        self.sigmoid = nn.Sigmoid()    # Сигмоїдна функція активації\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = self.sigmoid(out)        # Застосування сигмоїди\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Створюємо модель\n",
        "model = LogReg()\n",
        "\n",
        "# Виводимо структуру моделі\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn5teY_yvBPr",
        "outputId": "bcf4eabd-8b7b-4f6c-fe7a-3b8e877ecd22"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми створили клас LogReg, який наслідує властивості класу nn.Module.\n",
        "\n",
        "В конструкторі __init__ ми визначили два шари:\n",
        "\n",
        "self.linear — лінійний шар, який приймає на вхід 3 характеристики й видає одне значення.\n",
        "\n",
        "self.sigmoid — сигмоїдна функція активації, яка перетворює вихід у значення в межах від 0 до 1.\n",
        "\n",
        "У методі forward ми описали, що вхідні дані спочатку проходять через лінійний шар, а потім через сигмоїду для отримання ймовірності належності до класу.\n",
        "\n",
        "Ми створили екземпляр класу LogReg в змінній model.\n"
      ],
      "metadata": {
        "id": "KrZs7fhMsrhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Оптимізатор\n",
        "opt = optim.SGD(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція втрат\n",
        "loss_fn = F.binary_cross_entropy\n",
        "\n",
        "# Обчислення втрат\n",
        "loss = loss_fn(model(inputs), targets)\n",
        "\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_OjZdI5wssj",
        "outputId": "b941c971-2f91-43f8-fc0a-ab74276d85ad"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.6312, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сума втрат 7.6312 є досить високою, що вказує на те, що модель ще не навилась добре. Модель передбачає результати, які значно відрізняються від істинних міток."
      ],
      "metadata": {
        "id": "47k1wkLkwVFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Навчання моделі\n",
        "num_epochs = 1000\n",
        "losses = fit_return_loss(num_epochs, model, loss_fn, opt, train_dl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqP8JXj_zTJN",
        "outputId": "0d529f53-eade-43b8-d52e-a1a597f11779"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 6.6400\n",
            "Epoch [20/1000], Loss: 6.0483\n",
            "Epoch [30/1000], Loss: 5.7263\n",
            "Epoch [40/1000], Loss: 5.4710\n",
            "Epoch [50/1000], Loss: 5.2474\n",
            "Epoch [60/1000], Loss: 5.0653\n",
            "Epoch [70/1000], Loss: 4.8974\n",
            "Epoch [80/1000], Loss: 4.7861\n",
            "Epoch [90/1000], Loss: 4.6903\n",
            "Epoch [100/1000], Loss: 4.5353\n",
            "Epoch [110/1000], Loss: 4.4753\n",
            "Epoch [120/1000], Loss: 4.3686\n",
            "Epoch [130/1000], Loss: 4.2792\n",
            "Epoch [140/1000], Loss: 4.2018\n",
            "Epoch [150/1000], Loss: 4.1132\n",
            "Epoch [160/1000], Loss: 4.0389\n",
            "Epoch [170/1000], Loss: 3.9423\n",
            "Epoch [180/1000], Loss: 3.8701\n",
            "Epoch [190/1000], Loss: 3.7756\n",
            "Epoch [200/1000], Loss: 3.6974\n",
            "Epoch [210/1000], Loss: 3.6101\n",
            "Epoch [220/1000], Loss: 3.5297\n",
            "Epoch [230/1000], Loss: 3.4427\n",
            "Epoch [240/1000], Loss: 3.3760\n",
            "Epoch [250/1000], Loss: 3.2822\n",
            "Epoch [260/1000], Loss: 3.1985\n",
            "Epoch [270/1000], Loss: 3.1111\n",
            "Epoch [280/1000], Loss: 3.0308\n",
            "Epoch [290/1000], Loss: 2.9483\n",
            "Epoch [300/1000], Loss: 2.8748\n",
            "Epoch [310/1000], Loss: 2.7879\n",
            "Epoch [320/1000], Loss: 2.7048\n",
            "Epoch [330/1000], Loss: 2.6292\n",
            "Epoch [340/1000], Loss: 2.5456\n",
            "Epoch [350/1000], Loss: 2.4710\n",
            "Epoch [360/1000], Loss: 2.4122\n",
            "Epoch [370/1000], Loss: 2.3102\n",
            "Epoch [380/1000], Loss: 2.2363\n",
            "Epoch [390/1000], Loss: 2.1587\n",
            "Epoch [400/1000], Loss: 2.0854\n",
            "Epoch [410/1000], Loss: 2.0062\n",
            "Epoch [420/1000], Loss: 1.9342\n",
            "Epoch [430/1000], Loss: 1.8632\n",
            "Epoch [440/1000], Loss: 1.7922\n",
            "Epoch [450/1000], Loss: 1.7232\n",
            "Epoch [460/1000], Loss: 1.6485\n",
            "Epoch [470/1000], Loss: 1.5853\n",
            "Epoch [480/1000], Loss: 1.5150\n",
            "Epoch [490/1000], Loss: 1.4520\n",
            "Epoch [500/1000], Loss: 1.3831\n",
            "Epoch [510/1000], Loss: 1.3171\n",
            "Epoch [520/1000], Loss: 1.2595\n",
            "Epoch [530/1000], Loss: 1.2084\n",
            "Epoch [540/1000], Loss: 1.1388\n",
            "Epoch [550/1000], Loss: 1.0851\n",
            "Epoch [560/1000], Loss: 1.0415\n",
            "Epoch [570/1000], Loss: 0.9843\n",
            "Epoch [580/1000], Loss: 0.9393\n",
            "Epoch [590/1000], Loss: 0.8865\n",
            "Epoch [600/1000], Loss: 0.8436\n",
            "Epoch [610/1000], Loss: 0.8030\n",
            "Epoch [620/1000], Loss: 0.7632\n",
            "Epoch [630/1000], Loss: 0.7308\n",
            "Epoch [640/1000], Loss: 0.7033\n",
            "Epoch [650/1000], Loss: 0.6679\n",
            "Epoch [660/1000], Loss: 0.6437\n",
            "Epoch [670/1000], Loss: 0.6134\n",
            "Epoch [680/1000], Loss: 0.5918\n",
            "Epoch [690/1000], Loss: 0.5703\n",
            "Epoch [700/1000], Loss: 0.5492\n",
            "Epoch [710/1000], Loss: 0.5325\n",
            "Epoch [720/1000], Loss: 0.5149\n",
            "Epoch [730/1000], Loss: 0.5012\n",
            "Epoch [740/1000], Loss: 0.4870\n",
            "Epoch [750/1000], Loss: 0.4743\n",
            "Epoch [760/1000], Loss: 0.4654\n",
            "Epoch [770/1000], Loss: 0.4552\n",
            "Epoch [780/1000], Loss: 0.4421\n",
            "Epoch [790/1000], Loss: 0.4315\n",
            "Epoch [800/1000], Loss: 0.4263\n",
            "Epoch [810/1000], Loss: 0.4149\n",
            "Epoch [820/1000], Loss: 0.4085\n",
            "Epoch [830/1000], Loss: 0.4013\n",
            "Epoch [840/1000], Loss: 0.3940\n",
            "Epoch [850/1000], Loss: 0.3883\n",
            "Epoch [860/1000], Loss: 0.3829\n",
            "Epoch [870/1000], Loss: 0.3770\n",
            "Epoch [880/1000], Loss: 0.3719\n",
            "Epoch [890/1000], Loss: 0.3672\n",
            "Epoch [900/1000], Loss: 0.3627\n",
            "Epoch [910/1000], Loss: 0.3587\n",
            "Epoch [920/1000], Loss: 0.3549\n",
            "Epoch [930/1000], Loss: 0.3522\n",
            "Epoch [940/1000], Loss: 0.3474\n",
            "Epoch [950/1000], Loss: 0.3445\n",
            "Epoch [960/1000], Loss: 0.3411\n",
            "Epoch [970/1000], Loss: 0.3395\n",
            "Epoch [980/1000], Loss: 0.3350\n",
            "Epoch [990/1000], Loss: 0.3319\n",
            "Epoch [1000/1000], Loss: 0.3297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На початку тренування сума втрат висока, але поступово зменшується, що свідчить про поступове покращення моделі."
      ],
      "metadata": {
        "id": "_DQzBaqyxWjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Побудова графіку втрат\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Зміна втрат під час тренування')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ToTqI4Ms0qIK",
        "outputId": "ae1d88dc-4fce-4403-81c2-15793734103a"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На графіку ми можемо візуально побачити зміну суми втрат протягом тренування. Модель стабільно зменшує втрати під час навчання і після 900 епохи починає виходити на плато."
      ],
      "metadata": {
        "id": "AHbFKN-Ax8Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Фінальні передбачення\n",
        "preds = model(inputs)\n",
        "print(\"\\nПередбачення:\\n\", preds.detach())\n",
        "print(\"\\nТаргети:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npMyhDHs0Y6Z",
        "outputId": "8d91d16c-43a0-4250-b885-f65b4394d4c0"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Передбачення:\n",
            " tensor([[0.5729],\n",
            "        [0.6791],\n",
            "        [0.9026],\n",
            "        [0.1569],\n",
            "        [0.8761],\n",
            "        [0.5729],\n",
            "        [0.6791],\n",
            "        [0.9026],\n",
            "        [0.1569],\n",
            "        [0.8761],\n",
            "        [0.5729],\n",
            "        [0.6791],\n",
            "        [0.9026],\n",
            "        [0.1569],\n",
            "        [0.8761]])\n",
            "\n",
            "Таргети:\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Округлимо ймовірності, бо нам потрібні класи 0 або 1\n",
        "y_pred_class = (preds >= 0.5).float()\n",
        "print(\"Передбачені класи:\\n\", y_pred_class)\n",
        "print(\"Справжні класи (targets):\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNB_g2JB2xmm",
        "outputId": "c7f392b5-c155-4613-d9ac-a1d25b247fc9"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Передбачені класи:\n",
            " tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Справжні класи (targets):\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(targets, y_pred_class)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTzBWYQq30Gx",
        "outputId": "dbc5a40f-6e9b-4226-c902-e3ed034d3d01"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[3 3]\n",
            " [0 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "З початкових 6.64 на 10-й епосі, до кінця тренування сума втрат знизилася до 0.3297. Це свідчить про те, що модель навчалася і з кожною ітерацією покращувала свої передбачення.\n",
        "\n",
        "Модель показала досить хороші результати, однак є деякі помилки: 3 приклади класу 0 модель помилково класифікувала як клас 1. З іншого боку, жоден приклад класу 1 не був помилково класифікований як клас 0.\n"
      ],
      "metadata": {
        "id": "i2JeAFNlz5yk"
      }
    }
  ]
}