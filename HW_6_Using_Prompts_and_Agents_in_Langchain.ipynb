{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Завдання 1: Виклик LLM з базовим промптом\n",
        "\n",
        "Створіть можливість викликати LLM зі звичайним текстовим промптом.\n",
        "\n",
        "Промпт має дозвляти отримати інформацію простою мовою на певну тему. В цьому завданні ми хочемо дізнатись про тему \"Квантові обчислення\".\n",
        "\n",
        "Відповідь моделі повинна містити визначення, ключові переваги та поточні дослідження в цій галузі.\n",
        "\n",
        "Обмежте відповідь до 200 символів і пропишіть в промпті аби відповідь була короткою (це зекономить Вам час і гроші на згенеровані токени).\n",
        "\n",
        "В якості LLM можна скористатись як моделлю з HugginFace (рекомендую Mistral), так і ChatGPT4 або ChatGPT3. В обох випадках треба імпортувати потрібну \"обгортку\" (тобто клас, який дозволить ініціювати модель) з LangChain для виклику LLM за API, а також зчитати особистий токен з файла, наприклад, `creds.json`, який розміщений у Вас локально і Ви НЕ здаєте його в ДЗ і НЕ комітите в git 😏\n",
        "\n",
        "Встановіть своє значення температури на свій розсуд (тут немає правильного чи неправильного значення) і напишіть, чому ви обрали саме таке значення для цього завдання.  \n",
        "\n",
        "Запити можна робити як українською, так і англійською - орієнтуйтесь на те, де і чи хочете ви потім лишити цей проєкт і відповідна яка мова буде пасувати більше. В розвʼязках промпти - українською."
      ],
      "metadata": {
        "id": "RMalK_oYR-X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain_openai huggingface_hub openai"
      ],
      "metadata": {
        "id": "vT1ICe74cAUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6296ee8c-6639-4f55-a77e-b77409652b5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/420.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/420.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade langchain langchain-community openai huggingface_hub"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdVfW766BpFs",
        "outputId": "63ffc6a4-2b73-4120-ae9b-1ff966119dee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.68.2)\n",
            "Collecting openai\n",
            "  Downloading openai-1.69.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.69.0-py3-none-any.whl (599 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, openai, dataclasses-json, langchain-community\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.68.2\n",
            "    Uninstalling openai-1.68.2:\n",
            "      Successfully uninstalled openai-1.68.2\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 openai-1.69.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.llms import OpenAI  # Для OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n"
      ],
      "metadata": {
        "id": "zW7Ku8Zh_YgO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Зчитуємо API-ключі з файлу\n",
        "with open(\"creds.json\", \"r\") as file:\n",
        "    creds = json.load(file)\n",
        "    openai_api_key = creds[\"OPENAI_API_KEY\"]\n",
        ""
      ],
      "metadata": {
        "id": "e6UWT4Ll_dlq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overal_temperature = 0.2"
      ],
      "metadata": {
        "id": "FqNUQd6Y_x9F"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Промпт\n",
        "prompt = \"Explain quantum computing simply, covering definition, key advantages, and current research. Keep the response concise (max 200 characters).\""
      ],
      "metadata": {
        "id": "tT3Ca17h_2V3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_openai():\n",
        "    llm_openai = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=overal_temperature, openai_api_key=openai_api_key)\n",
        "    response = llm_openai.invoke(prompt)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "jslZEBY7AW9Z"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Використання OpenAI\n",
        "response_openai = query_openai()\n"
      ],
      "metadata": {
        "id": "6rL8nvJ5Aclm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"OpenAI GPT-4o-mini response:\", response_openai)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCEWwABDQU_i",
        "outputId": "3aac9010-78ab-4092-b6d6-7374a8b82efb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI GPT-4o-mini response: content='Quantum computing uses quantum bits (qubits) to perform calculations faster than classical computers. Key advantages include solving complex problems and optimization. Current research focuses on error correction and practical applications.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 32, 'total_tokens': 69, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_b376dfbbd5', 'finish_reason': 'stop', 'logprobs': None} id='run-fe68cfd8-7670-4118-9ad7-5789963cb6e3-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_openai.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQwAgdOPSyKw",
        "outputId": "36beb3d8-2b65-4b67-9609-bffe3b80bbf4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantum computing uses quantum bits (qubits) to perform calculations faster than classical computers. Key advantages include solving complex problems and optimization. Current research focuses on error correction and practical applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми визначили параметр температури (overal_temperature = 0.2), тобто обрали низьку температуру для моделі, щоб отримати отримати чітку та коротку відповідь без зайвих деталей.\n",
        "\n",
        "Встановили обмеження в 200 символів щоб забезпечити коротку відповідь.\n",
        "\n",
        "Викликали модель gpt-4o-mini через створену функцію query_openai() та отримали відповідь:\n",
        "\n",
        "\n",
        "Quantum computing uses quantum bits (qubits) to perform calculations faster than classical computers. Key advantages include solving complex problems and optimization. Current research focuses on error correction and practical applications.\n",
        "\n",
        "\n",
        "Висновок:\n",
        "\n",
        "Модель відповіла згідно з вимогами.\n",
        "\n"
      ],
      "metadata": {
        "id": "GDejcdb3T4zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання 2: Створення параметризованого промпта для генерації тексту\n",
        "Тепер ми хочемо оновити попередній фукнціонал так, аби в промпт ми могли передавати тему як параметр. Для цього скористайтесь `PromptTemplate` з `langchain` і реалізуйте параметризований промпт та виклик моделі з ним.\n",
        "\n",
        "Запустіть оновлений функціонал (промпт + модел) для пояснень про теми\n",
        "- \"Баєсівські методи в машинному навчанні\"\n",
        "- \"Трансформери в машинному навчанні\"\n",
        "- \"Explainable AI\"\n",
        "\n",
        "Виведіть результати відпрацювання моделі на екран."
      ],
      "metadata": {
        "id": "UiIzV0UIS0GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "kcg7EfdNcBDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_openai_params(topic):\n",
        "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.5, openai_api_key=openai_api_key)\n",
        "    prompt = prompt_template.format(topic=topic)\n",
        "    response = llm.invoke(prompt)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "XKZTJUmINB5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаблон промпту\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} simply, covering definition, key advantages, and current research. Keep the response concise (max 200 characters).\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "cF2FW0adCmNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Теми\n",
        "topics = [\n",
        "    \"Bayesian methods in machine learning\",\n",
        "    \"Transformers in machine learning\",\n",
        "    \"Explainable AI\"\n",
        "]\n",
        "\n",
        "# Виконання запитів\n",
        "for topic in topics:\n",
        "    response_openai = query_openai(topic)\n",
        "    print(f\"\\nTopic: {topic}\")\n",
        "    print(\"OpenAI GPT-4o-mini response:\", response_openai)\n"
      ],
      "metadata": {
        "id": "6ECrY0VhBqv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead665b0-89bd-4ab3-8e84-6f31f153ed70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic: Bayesian methods in machine learning\n",
            "OpenAI GPT-4o_mini response: content='Bayesian methods in ML use probability theory to update knowledge after considering evidence. Advantages include handling uncertainty, and incorporating prior knowledge. Current research focuses on improving computational efficiency.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 36, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1d42efab-7b77-4af9-9bb6-4dbbcd1dfec5-0'\n",
            "\n",
            "Topic: Transformers in machine learning\n",
            "OpenAI GPT-4o_mini response: content='Transformers are ML models that handle sequential data effectively. They offer parallel computing, improved accuracy, and better handling of long sequences. Current research focuses on optimization.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 35, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-f07a408a-be3e-4161-a418-55837934989d-0'\n",
            "\n",
            "Topic: Explainable AI\n",
            "OpenAI GPT-4o_mini response: content='Explainable AI is AI tech that provides clear, understandable explanations for its decisions. It promotes transparency, trust, and easier debugging. Current research focuses on improving its clarity.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 34, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-ef149703-6fb6-4772-ab7d-141ac36a8949-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми створили шаблон промпту (PromptTemplate) і в шаблоні передбачили змінну {topic}, яка дозволяє підставляти різні теми.\n",
        "\n",
        "МИ налаштували промпт так, щоб відповідь була короткою (до 200 символів) і містила визначення, переваги та поточні дослідження.\n",
        "\n",
        "Використали модель gpt-4o-mini з температурою 0.5, що забезпечує баланс між точністю і креативністю.\n",
        "\n",
        "\n",
        "Висновок:\n",
        "\n",
        "Всі відповіді відповідають вимогам: відповіді короткі, структуровані та зрозумілі."
      ],
      "metadata": {
        "id": "0YSNVG8OWpv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Завдання 3: Використання агента для автоматизації процесів\n",
        "Створіть агента, який допоможе автоматично шукати інформацію про останні наукові публікації в різних галузях. Наприклад, агент має знайти 5 останніх публікацій на тему штучного інтелекту.\n",
        "\n",
        "**Кроки:**\n",
        "1. Налаштуйте агента типу ReAct в LangChain для виконання автоматичних запитів.\n",
        "2. Створіть промпт, який спрямовує агента шукати інформацію в інтернеті або в базах даних наукових публікацій.\n",
        "3. Агент повинен видати список публікацій, кожна з яких містить назву, авторів і короткий опис.\n",
        "\n",
        "Для взаємодії з пошуком там необхідно створити `Tool`. В лекції ми використовували `serpapi`. Можна продовжити користуватись ним, або обрати інше АРІ для пошуку (вони в тому числі є безкоштовні). Перелік різних АРІ, доступних в langchain, і орієнтир по вартості запитів можна знайти в окремому документі [тут](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
        "\n",
        "Лишаю також нижче приклад використання одного з безкоштовних пошукових АРІ - DuckDuckGo (не потребує створення токена!)  - можливо він вам сподобається :)\n"
      ],
      "metadata": {
        "id": "m9UsL2gXSe-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-search-results langchain-community langchain_experimental"
      ],
      "metadata": {
        "id": "4E3woACWkYJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c80f56-fc3f-4ced-df27-8a7ef461bcdc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.tools import Tool\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Зчитуємо API-ключі\n",
        "with open(\"creds.json\", \"r\") as file:\n",
        "    creds = json.load(file)\n",
        "    openai_api_key = creds[\"OPENAI_API_KEY\"]\n",
        "    serpapi_key = creds[\"SERPAPI_API_KEY\"]\n",
        "\n",
        "# Налаштування пошукового інструменту SerpAPI\n",
        "search = SerpAPIWrapper(serpapi_api_key=serpapi_key)\n",
        "\n",
        "def search_papers(query):\n",
        "    \"\"\"Функція для пошуку наукових публікацій за допомогою SerpAPI.\"\"\"\n",
        "    results = search.run(f\"{query} site:arxiv.org OR site:researchgate.net OR site:dl.acm.org OR site:ieeexplore.ieee.org OR site:scholar.google.com\")\n",
        "    return results\n",
        "\n",
        "search_tool = Tool(\n",
        "    name=\"Research Paper Search\",\n",
        "    func=search_papers,\n",
        "    description=\"Шукає останні наукові статті на задану тему.\"\n",
        ")\n",
        "\n",
        "# Ініціалізація агента ReAct\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3, openai_api_key=openai_api_key)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=[search_tool],\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Виконання запиту\n",
        "topic = \"Artificial Intelligence\"\n",
        "response = agent.run(f\"Find the 5 most recent scientific papers on {topic} and give a response including their title, year, author of paper, and a short summary.\")\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLp_Zlg_C1XD",
        "outputId": "6eb0a224-0c55-410b-d59c-bd3941560a00"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to use the Research Paper Search tool to find the most recent scientific papers on Artificial Intelligence.\n",
            "Action: Research Paper Search\n",
            "Action Input: Artificial Intelligence\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m['LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning · Cognitive Science-Inspired Evaluation of Core ...', 'A common definition of AI is that it is a technology that enables machines to imitate various complex human skills. This, however, does not give ...', 'No information is available for this page.', 'Abstract:Artificial intelligence commonly refers to the science and engineering of artificial systems that can carry out tasks generally ...', 'Artificial Intelligence · 1. Neural Information Processing Systems · 2. International Conference on Learning Representations · 3. International Conference on ...', 'IEEE Transactions on Artificial Intelligence. The IEEE Transactions on Artificial Intelligence (TAI) is a multidisciplinary journal publishing paper.', 'Artificial intelligence (AI) is best understood as a special branch of computer science. Historically, the boundary between AI and computer science was in fact ...', 'Discussion of AGI is a revival of the early dream of a human-like intelligence, from which the AI community had gradually distanced itself, ...', 'We developed a novel early childhood artificial intelligence (AI) platform, PopBots, where preschool children train and interact with social robots to learn ...']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe search results are not providing the specific information I need. I need to refine my search to specifically look for recent scientific papers.\n",
            "Action: Research Paper Search\n",
            "Action Input: Recent scientific papers on Artificial Intelligence\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m['Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI. Danaja Rutar, Alva Markelius, Konstantinos Voudouris, José ...', 'Authors and titles for March 2025 · Agentic AI Needs a Systems Theory · Human-AI Collaboration: Trade-offs Between Performance and Preferences · Instructor-Worker ...', 'We find that current GenAI can make only incremental discoveries but cannot achieve fundamental discoveries from scratch as humans can.', 'This paper studies the impact of artificial intelligence on innovation, exploiting the randomized introduction of a new materials discovery technology.', 'The Journal of Artificial Intelligence Research (www.jair.org) covers all areas of artificial intelligence, publishing refereed research articles, survey ...', 'This paper gives an overview of this technology and the scope of artificial intelligence in different areas with special reference to the use of this ...', 'Machine Learning. Authors and titles for recent submissions. Fri, 28 Mar 2025 · Thu, 27 Mar 2025 · Wed, 26 Mar 2025 · Tue, 25 Mar 2025 · Mon, 24 Mar 2025.', 'This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research ...', 'A profound reflection on the impact of these innovative systems on scientific research and society at large.', 'Artificial Intelligence | Explore the latest full-text research PDFs, articles, conference papers, preprints and more on ARTIFICIAL INTELLIGENCE.']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe search results now include recent scientific papers on Artificial Intelligence. I will select the five most recent ones and provide their title, year, author, and a short summary.\n",
            "Final Answer: \n",
            "1. \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\" by Danaja Rutar, Alva Markelius, Konstantinos Voudouris, José ..., 2025. This paper evaluates the core capabilities of AI in understanding objects, drawing inspiration from cognitive science.\n",
            "2. \"Agentic AI Needs a Systems Theory\", March 2025. This paper argues that for AI to be truly agentic, it needs to be grounded in a systems theory.\n",
            "3. \"Human-AI Collaboration: Trade-offs Between Performance and Preferences\", March 2025. This paper explores the trade-offs between performance and preferences in human-AI collaboration.\n",
            "4. \"Impact of artificial intelligence on innovation\", 2025. This paper studies the impact of artificial intelligence on innovation, focusing on the introduction of a new materials discovery technology.\n",
            "5. \"Comprehensive Framework for Fully Automatic Scientific Discovery\", 2025. This paper presents a comprehensive framework for fully automatic scientific discovery, enabling large language models to perform research.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1. \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\" by Danaja Rutar, Alva Markelius, Konstantinos Voudouris, José ..., 2025. This paper evaluates the core capabilities of AI in understanding objects, drawing inspiration from cognitive science.\n",
            "2. \"Agentic AI Needs a Systems Theory\", March 2025. This paper argues that for AI to be truly agentic, it needs to be grounded in a systems theory.\n",
            "3. \"Human-AI Collaboration: Trade-offs Between Performance and Preferences\", March 2025. This paper explores the trade-offs between performance and preferences in human-AI collaboration.\n",
            "4. \"Impact of artificial intelligence on innovation\", 2025. This paper studies the impact of artificial intelligence on innovation, focusing on the introduction of a new materials discovery technology.\n",
            "5. \"Comprehensive Framework for Fully Automatic Scientific Discovery\", 2025. This paper presents a comprehensive framework for fully automatic scientific discovery, enabling large language models to perform research.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналіз результатів:\n",
        "\n",
        "Позитивні моменти:\n",
        "- Знайдено 5 релевантних статей із чіткими заголовками та коротким описом.\n",
        "- Статті актуальні (2025 рік), що свідчить про ефективний пошук останніх публікацій.\n",
        "- Охоплено різні аспекти ШІ – когнітивний підхід, агентність, співпраця з людьми, інновації та автоматизовані відкриття.\n",
        "\n",
        "\n",
        "Можливі покращення:\n",
        "- Відсутні повні списки авторів у деяких статтях – можна додати запит на пошук через Semantic Scholar API, наприклад.\n",
        "- Агент міг би зібрати посилання на статті для зручності користувача.\n",
        "- Деякі описи узагальнені – можна покращити витягування основних висновків із тексту статті.\n",
        "\n"
      ],
      "metadata": {
        "id": "_e4XzWe5fa8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Завдання 4: Створення агента-помічника для вирішення бізнес-задач\n",
        "\n",
        "Створіть агента, який допомагає вирішувати задачі бізнес-аналітики. Агент має допомогти користувачу створити прогноз по продажам на наступний рік враховуючи рівень інфляції і погодні умови. Агент має вміти використовувати Python і ходити в інтернет аби отримати актуальні дані.\n",
        "\n",
        "**Кроки:**\n",
        "1. Налаштуйте агента, який працюватиме з аналітичними даними, заданими текстом. Користувач пише\n",
        "\n",
        "```\n",
        "Ми експортуємо апельсини з Бразилії. В 2021 експортували 200т, в 2022 - 190т, в 2023 - 210т, в 2024 який ще не закінчився - 220т. Зроби оцінку скільки ми зможемо експортувати апельсинів в 2025 враховуючи погодні умови в Бразилії і попит на апельсини в світі виходячи з економічної ситуації.\n",
        "```\n",
        "\n",
        "2. Створіть запит до агента, що містить чітке завдання – видати результат бізнес аналізу або написати, що він не може цього зробити і запит користувача (просто може бути все одним повідомлленням).\n",
        "\n",
        "3. Запустіть агента і проаналізуйте результати. Що можна покращити?\n"
      ],
      "metadata": {
        "id": "IOqujC6qY_NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-search-results langchain-community langchain_experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylkef5l-ZerH",
        "outputId": "e9ebc2d5-2a1e-44c7-df8a-ffa96ce3eeaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai google-search-results pandas requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e4Wy8CxhFE-h",
        "outputId": "faf1cde8-663d-4084-a4ae-385234a123e5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.69.0)\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.11/dist-packages (2.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "\n",
        "# Зчитуємо API-ключі\n",
        "with open(\"creds.json\", \"r\") as file:\n",
        "    creds = json.load(file)\n",
        "    openai_api_key = creds[\"OPENAI_API_KEY\"]\n",
        "    serpapi_key = creds[\"SERPAPI_API_KEY\"]\n",
        "\n",
        "# Налаштування пошукового інструменту SerpAPI\n",
        "search = SerpAPIWrapper(serpapi_api_key=serpapi_key)\n",
        "\n",
        "\n",
        "# Economic Data Tool (Inflation, GDP, Trade)\n",
        "economic_data_tool = Tool(\n",
        "    name=\"Economic Data Search\",\n",
        "    func=lambda query: search.run(f\"{query} site:tradingeconomics.com OR site:reuters.com OR site:worldbank.org\"),\n",
        "    description=\"Search for economic indicators like inflation, GDP, and trade data.\"\n",
        ")\n",
        "\n",
        "# Weather Data Tool (Climate Trends, Droughts)\n",
        "weather_data_tool = Tool(\n",
        "    name=\"Weather Data Search\",\n",
        "    func=lambda query: search.run(f\"{query} site:weather.com OR site:climate.gov OR site:noaa.gov\"),\n",
        "    description=\"Search for weather conditions and climate trends.\"\n",
        ")"
      ],
      "metadata": {
        "id": "zI-7VSRmFE_5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "\n",
        "# Initialize OpenAI LLM (GPT-4)\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Create Business Analytics Agent\n",
        "agent = initialize_agent(\n",
        "    tools=[economic_data_tool, weather_data_tool, PythonREPLTool()],\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "5TueG93pFL-I"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "We export oranges from Brazil.\n",
        "- 2021: 200 tons\n",
        "- 2022: 190 tons\n",
        "- 2023: 210 tons\n",
        "- 2024 (incomplete): 220 tons\n",
        "Estimate the 2025 exports considering:\n",
        "1. Inflation in Brazil (use Economic Data Search)\n",
        "2. Weather conditions in Brazil (use Weather Data Search)\n",
        "3. Global demand for oranges\n",
        "Provide a clear forecast using Python or explain why you cannot make one.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = agent.run(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQGRZN6eFNxT",
        "outputId": "00051a45-898d-47e7-cf0b-eb9f805c90c8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mFirst, I need to gather the necessary data. I will start by searching for the inflation rate in Brazil for the years 2021-2024. Then, I will look for weather conditions in Brazil for the same years. Finally, I will need to make an assumption about the global demand for oranges, as I don't have a tool to provide this data. \n",
            "\n",
            "Action: Economic Data Search\n",
            "Action Input: Inflation rate in Brazil 2021-2024\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m['Annual inflation rate in Brazil came in at 4.83% in December 2024, little changed from 4.87% in November. Inflation softened for housing and utilities.', 'Inflation, consumer prices (annual %) - Brazil. International Monetary Fund, International Financial Statistics and data files.', 'Inflation Rate - By Country ; Brazil, 5.06, 4.56 ; Brunei, 0.01, -0.4 ; Bulgaria, 4, 3.7 ; Burkina Faso, 4.9, 3.6 ...', \"Brazil's GDP grew 0.9% from the previous quarter in the three months ending September of 2024, as expected, after a 1.4% advance in Q2.\", 'The unemployment rate in Brazil fell to 6.2% in the final quarter of 2024, easing from the 6.4% in the third quarter, but slightly ahead of market expectations ...', 'The Gross Domestic Product (GDP) in Brazil was worth 2173.67 billion US dollars in 2023, according to official data from the World Bank.', 'The annual inflation rate in the US edged up to 3% in January 2025, compared to 2.9% in December 2024, and above market forecasts of 2.9%.', 'The annual inflation rate in Turkey slowed to 42.12% in January 2025, down from 44.38% in December but above forecasts of 41.25%.', \"Australia's annual inflation rate fell to 2.4% in Q4 2024 from 2.8% in Q3, less than the market consensus of 2.5%. It was the lowest reading since Q1 2021.\", 'The annual inflation rate in Switzerland fell to 0.4% in January 2025, in line with market expectations and down from 0.6% in December.']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe inflation rate in Brazil for the years 2021-2024 seems to be around 4.5-5%. Now I will look for the weather conditions in Brazil for the same years.\n",
            "\n",
            "Action: Weather Data Search\n",
            "Action Input: Weather conditions in Brazil 2021-2024\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m['Drought conditions have severely impacted central and southern Brazil this year, resulting in one of the worst droughts for the nation in centuries.', 'Use the form below to select from a collection of monthly summaries recapping climate-related occurrences on both a global and national scale. Report:.', 'Above-average precipitation fell across much of Argentina to Paraguay, and parts of eastern Brazil, during August, but the SPI maps show northern, western, and ...', 'GDIS global indicators revealed dry conditions continued during February 2024 across northern and eastern parts of North America, Central America, Brazil to the ...', 'The patterns also reveal flooding conditions in northern. Brazil and continued drought conditions over much of southern Brazil, Paraguay, Uruguay, and.', 'It provides a detailed update on global climate indicators, notable weather events, and other data collected by environmental monitoring ...', 'Previous studies have suggested the existence of a link between North and South America intraseasonal variability with that in the Indo-West pacific basin.', 'Observations of surface ocean carbon concentrations provide information of the changes in ocean carbon uptake and ocean acidification, as well as provide.', 'We anticipate that structured mesh models will continue to be the predomi- nant choice for ocean climate modeling for at least another ... the abyssal brazil ...', 'AMOC has important impacts on coastal sea levels, extreme weather events, marine heat waves, and shifts in regional surface temperature and ...']\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mThe weather conditions in Brazil for the years 2021-2024 seem to have been affected by droughts, particularly in the central and southern regions. This could potentially impact the production of oranges. Now, I need to make an assumption about the global demand for oranges. Since I don't have specific data, I will assume that the demand remains constant. \n",
            "\n",
            "Now, I will use Python to calculate the average growth rate of orange exports from 2021 to 2024, and then apply this rate to the 2024 exports to estimate the 2025 exports. I will also consider the inflation rate and the impact of weather conditions on the production of oranges.\n",
            "\n",
            "Action: Python_REPL\n",
            "Action Input: \n",
            "```python\n",
            "# Define the exports data\n",
            "exports = [200, 190, 210, 220]\n",
            "\n",
            "# Calculate the average growth rate\n",
            "growth_rates = [(exports[i] - exports[i-1]) / exports[i-1] for i in range(1, len(exports))]\n",
            "avg_growth_rate = sum(growth_rates) / len(growth_rates)\n",
            "\n",
            "# Estimate the 2025 exports\n",
            "estimated_2025_exports = exports[-1] * (1 + avg_growth_rate)\n",
            "\n",
            "# Adjust for inflation\n",
            "inflation_rate = 0.05  # approximated from the data\n",
            "adjusted_2025_exports = estimated_2025_exports / (1 + inflation_rate)\n",
            "\n",
            "# Adjust for weather conditions\n",
            "# Assuming that drought reduces production by 10%\n",
            "weather_impact = 0.1\n",
            "final_2025_exports = adjusted_2025_exports * (1 - weather_impact)\n",
            "\n",
            "print(final_2025_exports)\n",
            "```\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m195.03831006086642\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The estimated exports of oranges from Brazil in 2025, considering the inflation rate, weather conditions, and assuming a constant global demand, is approximately 195 tons.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The estimated exports of oranges from Brazil in 2025, considering the inflation rate, weather conditions, and assuming a constant global demand, is approximately 195 tons.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Агент виявив, що інфляція в Бразилії у 2021-2024 роках коливалася в межах 4.5-5%. Він застосував цю інформацію для корекції прогнозу експорту.\n",
        "\n",
        "\n",
        "Агент виявив, що Бразилія стикалася з сильними посухами в центральних і південних регіонах.\n",
        "Він припустив, що це призведе до зниження врожаю на 10%.\n",
        "\n",
        "Далі агент розрахував середній річний темп зростання експорту за 2021-2024 роки, скоригував прогноз на 2025 рік з урахуванням інфляції та погодних умов.\n",
        "\n",
        "\n",
        "Остаточний прогноз: ≈195 тис. тонн у 2025 році.\n",
        "\n",
        "Розрахунки є зрозумілими та логічними, але вони спрощені:\n",
        "\n",
        "- Агент не враховує можливі зміни в глобальному попиті чи інших економічних чинниках, які можуть значно змінити прогноз.\n",
        "\n",
        "- Агент спирається на спрощене припущення щодо інфляції та погодних умов.\n",
        "\n",
        "Що можна покращити?\n",
        "1. Замість припущення про постійний попит – отримати реальні дані про світовий ринок апельсинів.\n",
        "2. Залучити додаткові фактори, наприклад:\n",
        "\n",
        "- Прогнози майбутніх погодних умов.\n",
        "- Конкуренція з іншими країнами-експортерами (Іспанія, Мексика)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lEodAhxwlY-F"
      }
    }
  ]
}
